{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ea0f86",
   "metadata": {},
   "source": [
    "# Workbook for filtering and plotting Boltz-2 output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import modelcif.reader\n",
    "import modelcif\n",
    "import biotite\n",
    "import biotite.structure as struc\n",
    "import biotite.structure.io as strucio\n",
    "from filtering_functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb22fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "\n",
    "folder_list = os.listdir('./') # Define the root folder where the boltz-2 output folders are located\n",
    "\n",
    "confidence_value = float(0.7) # Define the boltz-2 confidence value to filter results\n",
    "\n",
    "affinity_value = float(0.1) # Definte the boltz-2 affinity value to filter results (this is the predicted binding affinity, not the experimental one)\n",
    "\n",
    "##### May want to change these variables #####\n",
    "# 157 is Histadine, 89 is Tyrosine, 211 is Phe\n",
    "# Note: the distance array starts at 0, so amino acids numbers from chimerax will be +1\n",
    "\n",
    "binding_site_residue1 = int(157)\n",
    "binding_site_residue2 = None  # You can set this to None if you don't want to filter by a second residue\n",
    "binding_site_residue3 = None  # You can set this to None if you don't want to filter by a third residue\n",
    "distance_threshold = float(15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c13e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folder_list:\n",
    "    if folder.startswith('boltz_results_'):\n",
    "        predictions_folder = os.path.join(folder, 'predictions/', folder.split('_')[2])\n",
    "        if not os.path.exists(predictions_folder):\n",
    "            print(f\"Predictions folder does not exist: {predictions_folder}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"working on folder: {predictions_folder}\")\n",
    "        print(f\"Processing folder: {predictions_folder}\")  # Debugging statement\n",
    "        results_df = parse_boltz2_results(os.path.join('./', predictions_folder))\n",
    "        # For example, to save it as a CSV file:\n",
    "        results_df.to_csv(f\"{folder}_results.csv\", index=False)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Combine all the CSV files into a single DataFrame\n",
    "\n",
    "file_list = [f for f in os.listdir('./') if f.endswith('_results.csv')]\n",
    "\n",
    "combined_df = combine_csv_results(file_list)\n",
    "combined_df.to_csv('boltz_results_combined.csv', index=False)\n",
    "print(\"Combined results saved to 'boltz_results_combined.csv'\")  # Debugging statement\n",
    "\n",
    "# remove the temporary .csv files\n",
    "for file in file_list:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        #print(f\"Removed temporary file: {file}\")  # Debugging statement\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Copy all the .pdb files from the combined results to a new directory\n",
    "combined_output_dir = 'combined_models'\n",
    "os.makedirs(combined_output_dir, exist_ok=True)\n",
    "for file in combined_df['model_path'].unique():\n",
    "    if os.path.exists(file):\n",
    "        # Copy the model file to the output directory\n",
    "        os.system(f\"cp {file} {combined_output_dir}\")\n",
    "        print(f\"Copied {file} to {combined_output_dir}\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {file}\")  # Debugging statement\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hydrogen bonds between the Receptor and Ligand for each model and append the results to the DataFrame\n",
    "\n",
    "# Optional: Use fewer processes if memory is limited\n",
    "num_processes = min(mp.cpu_count(), 8)  # Cap at 8 processes\n",
    "\n",
    "# Populate pdb_file_list with paths to PDB files\n",
    "pdb_file_list = []\n",
    "for folder in folder_list:\n",
    "    if folder.startswith('boltz_results_'):\n",
    "        # Extract the numeric identifier from the folder name\n",
    "        folder_id = folder.split('_')[2]\n",
    "        predictions_folder = os.path.join(folder, 'predictions', folder_id)\n",
    "        if os.path.exists(predictions_folder):\n",
    "            for f in os.listdir(predictions_folder):\n",
    "                if f.endswith('.pdb'):\n",
    "                    pdb_file_path = os.path.join(predictions_folder, f)\n",
    "                    pdb_file_list.append(pdb_file_path)\n",
    "\n",
    "print(f\"Found {len(pdb_file_list)} PDB files to process\")\n",
    "\n",
    "# Process files in parallel\n",
    "num_processes = mp.cpu_count()  # Use all available CPU cores\n",
    "print(f\"Starting hydrogen bond analysis with {num_processes} processes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with mp.Pool(processes=num_processes) as pool:\n",
    "    results = pool.map(process_hydrogen_bonds, pdb_file_list)\n",
    "\n",
    "# Filter out None results and collect hydrogen bond data\n",
    "hydrogen_bond_data = [result for result in results if result is not None]\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Hydrogen bond analysis completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Successfully processed {len(hydrogen_bond_data)} files\")\n",
    "\n",
    "# Save the hydrogen bond data to a CSV file\n",
    "hydrogen_bond_df = pd.concat(hydrogen_bond_data, axis=1).transpose()\n",
    "\n",
    "# Rename the columns\n",
    "hydrogen_bond_df.columns = ['num_hbonds']  # Set the column name for the hydrogen bond count\n",
    "hydrogen_bond_df.index.name = 'model_path'  # Set the index name to 'model_path'\n",
    "\n",
    "hydrogen_bond_df.index = hydrogen_bond_df.index.map(lambda x: f\"./{x}\" if not x.startswith(\"./\") else x)\n",
    "\n",
    "hydrogen_bond_df.to_csv('hydrogen_bond_data.csv', index=True)\n",
    "print(\"Hydrogen bond data saved to 'hydrogen_bond_data.csv'\")\n",
    "\n",
    "# Combine the hydrogen bond data with the combined DataFrame\n",
    "combined_df = pd.merge(combined_df, hydrogen_bond_df, left_on='model_path', right_index=True, how='left')\n",
    "\n",
    "# Save the combined DataFrame with hydrogen bond data to a new CSV file\n",
    "combined_df.to_csv('boltz_results_combined_with_hbonds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance from the center of mass for each model for distance filtering\n",
    "\n",
    "# Filtering by distance of CA atoms to the COM of the ligand\n",
    "\n",
    "distance_filtered_dir = 'distance_filtered'\n",
    "os.makedirs(distance_filtered_dir, exist_ok=True)\n",
    "\n",
    "distance_data = [] # List to store distance data for each model\n",
    "\n",
    "for file_path in pdb_file_list:\n",
    "    if not os.path.isfile(file_path):  # Skip if it's not a file\n",
    "        print(f\"Skipping non-file: {file_path}\")  # Debugging statement\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        pdb_file_obj = pdb.PDBFile.read(file_path)\n",
    "        pdb_data = pdb_file_obj.get_structure()\n",
    "      \n",
    "        CA_positions = extract_ca_positions(pdb_data, chain_id='A')\n",
    "        Lig_positions = extract_atoms_by_chain(pdb_data, chain_id='B')\n",
    "        Lig_COM = calculate_center_of_mass(Lig_positions)\n",
    "        distances = calculate_distances_to_com(CA_positions, Lig_COM)\n",
    "\n",
    "        # Append the distances and filename to the list\n",
    "        distance_data.append(pd.Series(distances, name=file_path))\n",
    "\n",
    "        if filter_by_sites(distances, binding_site_residue1, binding_site_residue2, binding_site_residue3, distance_threshold):\n",
    "            # If the distances pass the filter, copy the file to the distance_filtered_dir\n",
    "            os.system(f\"cp {file_path} {distance_filtered_dir}\")\n",
    "            # print(f\"Copied {file_path} to {distance_filtered_dir}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Combine all the distance data into a single DataFrame using pd.concat\n",
    "distance_df = pd.concat(distance_data, axis=1).transpose()\n",
    "\n",
    "# Write the distance data to a CSV file\n",
    "distance_df.to_csv('distance_matrices.csv', index=True)\n",
    "\n",
    "# Rename the columns\n",
    "distance_df.index.name = 'model_path'  # Set the index name to 'model_path'\n",
    "distance_df.index = distance_df.index.map(lambda x: f\"./{x}\" if not x.startswith(\"./\") else x)\n",
    "\n",
    "# Reset the index to make 'model_path' a regular column\n",
    "distance_df = distance_df.reset_index()\n",
    "\n",
    "# Define the binding site residue column to extract\n",
    "binding_site_residue_column = binding_site_residue1  # Ensure the column name matches the residue identifier\n",
    "\n",
    "# Ensure the column exists in the DataFrame\n",
    "if binding_site_residue_column in distance_df.columns:\n",
    "    # Group by model_path and select the mean distance for the specified residue\n",
    "    reduced_distance_df = distance_df.groupby('model_path', as_index=False)[binding_site_residue_column].mean()\n",
    "    reduced_distance_df.rename(columns={binding_site_residue_column: 'distance_to_residue'}, inplace=True)\n",
    "\n",
    "    # Save the reduced DataFrame to a CSV file\n",
    "    reduced_distance_df.to_csv('reduced_distance_data.csv', index=False)\n",
    "    print(\"Reduced distance data saved to 'reduced_distance_data.csv'\")\n",
    "else:\n",
    "    print(f\"Column '{binding_site_residue_column}' not found in distance_df.\")\n",
    "\n",
    "# Combine the distance data with the combined DataFrame\n",
    "combined_df = pd.merge(combined_df, reduced_distance_df, on='model_path', how='left')\n",
    "combined_df.to_csv('boltz_results_combined_with_hbonds_distances.csv', index=False)\n",
    "print(\"Combined DataFrame with distances saved to 'boltz_results_combined_with_hbonds_distances.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b17369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of confidence scores and affinity prediction values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(combined_df['confidence_score'], bins=30, kde=True)\n",
    "plt.title('Distribution of Confidence Scores')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(combined_df['affinity_pred_value'], bins=30, kde=True)\n",
    "plt.title('Distribution of Affinity Prediction Values')\n",
    "plt.xlabel('Affinity Prediction Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('boltz_results_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of distances to specific residues\n",
    "# The distance data is expected to be in a CSV file named 'distance_matrices.csv'\n",
    "\n",
    "distance_data = pd.read_csv('distance_matrices.csv')\n",
    "\n",
    "# The distance data has the first column as the model name and each subsequent column as the distance to a specific residue.\n",
    "\n",
    "# Initialally just want to define the column to plot\n",
    "binding_site_residue1 = 157\n",
    "binding_site_residue2 = 90\n",
    "binding_site_residue3 = 212\n",
    "binding_site_residue4 = 57\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(distance_data.iloc[:, binding_site_residue1], bins=50, kde=True)\n",
    "plt.title(f'Distribution of Distances to Residue {binding_site_residue1}')\n",
    "plt.xlabel('Mean ligand distance to Residue (angstroms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(distance_data.iloc[:, binding_site_residue2], bins=50, kde=True)\n",
    "plt.title(f'Distribution of Distances to Residue {binding_site_residue2}')\n",
    "plt.xlabel('Mean ligand distance to Residue (angstroms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid() \n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(distance_data.iloc[:, binding_site_residue3], bins=50, kde=True)\n",
    "plt.title(f'Distribution of Distances to Residue {binding_site_residue3}')\n",
    "plt.xlabel('Mean ligand distance to Residue (angstroms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(distance_data.iloc[:, binding_site_residue4], bins=50, kde=True)\n",
    "plt.title(f'Distribution of Distances to Residue {binding_site_residue4}')\n",
    "plt.xlabel('Mean ligand distance to Residue (angstroms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distance_distribution_plots.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot each column of the distance data, ploting the mean and standard deviation of each column\n",
    "# This will help to visualize the distribution of distances to each residue across all models\n",
    "mean_distance = []\n",
    "std_distance = []\n",
    "\n",
    "for i in range(1, distance_data.shape[1]):\n",
    "    # Calculate and plot the mean and standard deviation\n",
    "    mean_distance.append(distance_data.iloc[:, i].mean())\n",
    "    std_distance.append(distance_data.iloc[:, i].std())\n",
    "\n",
    "# Create a DataFrame for the mean and standard deviation\n",
    "distance_summary = pd.DataFrame({\n",
    "    'Residue': distance_data.columns[1:],\n",
    "    'Mean Distance': mean_distance,\n",
    "    'Standard Deviation': std_distance\n",
    "})\n",
    "\n",
    "# Plot the mean and standard deviation\n",
    "plt.figure(figsize=(40, 10))\n",
    "plt.errorbar(distance_summary['Residue'], distance_summary['Mean Distance'], yerr=distance_summary['Standard Deviation'], fmt='o', capsize=5)\n",
    "plt.title('Mean and Standard Deviation of Distances to Residues')\n",
    "plt.xlabel('Residue')\n",
    "plt.ylabel('Mean Distance (angstroms)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('mean_std_distance_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Filter the distance_summary, and return the residue index that have the mean distance below a certain threshold\n",
    "distance_threshold = 12.0\n",
    "\n",
    "# Filter the distance data based on the distance threshold\n",
    "filtered_distance_data = distance_summary[\n",
    "    (distance_summary['Mean Distance'] < distance_threshold)    \n",
    "]\n",
    "\n",
    "\n",
    "# Save the filtered distance data to a CSV file\n",
    "filtered_distance_data.to_csv('filtered_distance_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the combined dataframe based on all the combined metrics\n",
    "# This will filter the DataFrame based on the confidence score, affinity prediction value, and distance\n",
    "filtered_combined_df = combined_df[\n",
    "    (combined_df['confidence_score'] > confidence_value) &\n",
    "    (combined_df['affinity_pred_value'] < affinity_value) &\n",
    "    (combined_df['distance_to_residue'] < distance_threshold)\n",
    "]\n",
    "\n",
    "filtered_combined_df.to_csv('Final_Filtered_models.csv', index=False)\n",
    "\n",
    "filtered_dir = 'filtered_models'\n",
    "os.makedirs(filtered_dir, exist_ok=True)\n",
    "\n",
    "for index, row in filtered_combined_df.iterrows():\n",
    "    model_path = row['model_path']\n",
    "    if os.path.exists(model_path):\n",
    "        # Copy the model file to the output directory\n",
    "        os.system(f\"cp {model_path} {filtered_dir}\")\n",
    "        print(f\"Copied {model_path} to {filtered_dir}\")\n",
    "    else:\n",
    "        print(f\"Model file does not exist: {model_path}\")  # Debugging statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffca2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the filtered models to the reference structure\n",
    "\n",
    "directory_to_parse = './filtered_models'  # Change to point to directory with your PDB files\n",
    "directory_to_save = './filtered_aligned' # Change to point to directory where you want to save aligned files\n",
    "num_processes = mp.cpu_count()  # Use all available CPU cores, or set to a specific number\n",
    "\n",
    "# Main execution\n",
    "print(f\"Starting parallel PDB alignment with {num_processes} processes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(directory_to_save):\n",
    "    os.makedirs(directory_to_save)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_to_parse):\n",
    "    print(f\"Error: The directory '{directory_to_parse}' does not exist.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check if the directory is empty\n",
    "if not os.listdir(directory_to_parse):\n",
    "    print(f\"Error: The directory '{directory_to_parse}' is empty.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Parse the PDB files in the directory\n",
    "pdb_files = [f for f in os.listdir(directory_to_parse) if f.endswith('.pdb')]\n",
    "if not pdb_files:\n",
    "    print(f\"Error: No PDB files found in the directory '{directory_to_parse}'.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if len(pdb_files) < 2:\n",
    "    print(\"Error: At least two PDB files are required for alignment.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"Found {len(pdb_files)} PDB files to process\")\n",
    "\n",
    "# Get the reference structure (first file) and extract CA coordinates\n",
    "parser = PDBParser(QUIET=True)\n",
    "reference_structure = parser.get_structure(\"reference\", os.path.join(directory_to_parse, pdb_files[0]))\n",
    "reference_ca_atoms = [atom for atom in reference_structure.get_atoms() if atom.get_id() == 'CA']\n",
    "reference_ca_coords = [atom.get_coord() for atom in reference_ca_atoms]\n",
    "\n",
    "print(f\"Reference structure: {pdb_files[0]} with {len(reference_ca_coords)} CA atoms\")\n",
    "\n",
    "# Save the reference structure (unchanged)\n",
    "io = PDBIO()\n",
    "io.set_structure(reference_structure)\n",
    "output_file_ref = os.path.join(directory_to_save, f\"aligned_{pdb_files[0]}\")\n",
    "io.save(output_file_ref)\n",
    "print(f\"Reference structure saved to {output_file_ref}\")\n",
    "\n",
    "# Prepare the partial function with fixed arguments\n",
    "align_func = partial(align_structure, \n",
    "                    reference_ca_coords=reference_ca_coords,\n",
    "                    directory_to_parse=directory_to_parse,\n",
    "                    directory_to_save=directory_to_save)\n",
    "\n",
    "# Process files in parallel (excluding the reference file)\n",
    "files_to_process = pdb_files[1:]\n",
    "print(f\"Processing {len(files_to_process)} files in parallel...\")\n",
    "\n",
    "with mp.Pool(processes=num_processes) as pool:\n",
    "    results = pool.map(align_func, files_to_process)\n",
    "\n",
    "# Print results\n",
    "successful_alignments = 0\n",
    "failed_alignments = 0\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    if \"Error\" in result:\n",
    "        failed_alignments += 1\n",
    "    else:\n",
    "        successful_alignments += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nAlignment process completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Processed {len(pdb_files)} files using {num_processes} parallel processes\")\n",
    "print(f\"Successful alignments: {successful_alignments}\")\n",
    "print(f\"Failed alignments: {failed_alignments}\")\n",
    "\n",
    "# Optional: Calculate alignment statistics\n",
    "if successful_alignments > 0:\n",
    "    print(\"\\nAlignment Statistics:\")\n",
    "    rmsd_values = []\n",
    "    for result in results:\n",
    "        if \"RMSD:\" in result:\n",
    "            try:\n",
    "                rmsd_str = result.split(\"RMSD: \")[1].split(\" Å\")[0]\n",
    "                rmsd_values.append(float(rmsd_str))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if rmsd_values:\n",
    "        import numpy as np\n",
    "        print(f\"  Mean RMSD: {np.mean(rmsd_values):.2f} Å\")\n",
    "        print(f\"  Min RMSD: {np.min(rmsd_values):.2f} Å\")\n",
    "        print(f\"  Max RMSD: {np.max(rmsd_values):.2f} Å\")\n",
    "        print(f\"  Std RMSD: {np.std(rmsd_values):.2f} Å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e480477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data and zip results for sharing:\n",
    "\n",
    "# Create ZIP archive of the important results for sharing\n",
    "# Path to the existing archive\n",
    "archive_name = f'boltz_results_archive_confidence{confidence_value}_affinity{affinity_value}'\n",
    "\n",
    "# Temporary directory to extract the archive\n",
    "temp_dir = 'temp_archive'\n",
    "\n",
    "# Copy the .csv files to the temporary directory\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "shutil.copy('boltz_results_combined_with_hbonds_distances.csv', temp_dir)\n",
    "shutil.copy('distance_matrices.csv', temp_dir)\n",
    "shutil.copy('Final_Filtered_models.csv', temp_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Copy the filtered_models directory to the temporary directory\n",
    "filtered_models_path = os.path.join(temp_dir, 'filtered_models')\n",
    "shutil.copytree('filtered_models', filtered_models_path, dirs_exist_ok=True)\n",
    "\n",
    "# Copy the combined_models directory to the temporary directoryxist_ok=True)\n",
    "combined_models_path = os.path.join(temp_dir, 'combined_models')\n",
    "shutil.copytree('combined_models', combined_models_path, dirs_exist_ok=True)\n",
    "\n",
    "# Copy the aligned models directory to the temporary directory\n",
    "aligned_models_path = os.path.join(temp_dir, 'filtered_aligned')\n",
    "shutil.copytree('filtered_aligned', aligned_models_path, dirs_exist_ok=True)\n",
    "\n",
    "# Copy all the .png files to the temporary directory\n",
    "png_files = [f for f in os.listdir('.') if f.endswith('.png')]\n",
    "for png_file in png_files:\n",
    "    shutil.copy(png_file, temp_dir)\n",
    "\n",
    "# Create the archive with the updated contentsdels_path, dirs_exist_ok=True)\n",
    "shutil.make_archive(archive_name, 'zip', temp_dir)\n",
    "# Create the archive with the updated contents\n",
    "# Clean up the temporary directory'zip', temp_dir)\n",
    "shutil.rmtree(temp_dir)\n",
    "# Clean up the temporary directory\n",
    "print(\"Results Filtered and Collated comrade\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boltz2-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
